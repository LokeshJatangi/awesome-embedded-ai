---
layout: default
---

# 嵌入式AI简报 (2020-04-04)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本期18条。「新闻」部分三个最近开源的框架：清华大学计算机图形学组的Jittor、[旷视动态图和静态图合一且训练推理一体的MegEngine（ARM部分6月开源）](https://mp.weixin.qq.com/s/wGGFZJOxgJVzf30ZrfI2lQ)、[华为自动实现分布式并行训练的MindSpore](https://mp.weixin.qq.com/s/fgGOtAxLmP0huZOrE5m74w)，也期待今年上半年将要开源的OneFlow，华为最近也推出了基于完整解决方案的[华为云ModelArts Pro](https://mp.weixin.qq.com/s/CfFVxD6fYKA-J9tka4eg2g)。  
「论文」GAN压缩基于训练方案+NAS，信息保留的二值神经网络，落地性能和实用性俱佳的IR-Net，超越EfficientNet的RegNet。 
「开源」Google发布了更轻量的EfficienctNet-Lite，百度飞桨发布视觉分割库PaddleSeg并可结合Slim工具做压缩，Android Valgrind工具等等。  
「博文」仍旧是微信在小网络设计方面的最佳实践（是上期的前篇），以及两篇关于深度学习框架的灵魂和如何欣赏一个深度学习框架，非常值得阅读。  
硬件厂商这边，[三星目前正与超微半导体公司（AMD）合作发新的定制GPU解决方案，把Radeon GPU技术应用到Exynos芯片中](https://mp.weixin.qq.com/s/Fs0py4zuTH8_wZ4qSYWoeg)。但是，Exynos 990与骁龙865在CPU/GPU性能上差距明显，导致[数万网友联名请愿：要求三星停用Exynos处理器](https://mp.weixin.qq.com/s/6dcZUbjmaN5s4vEAN8FR0Q)。不仅麒麟820，[麒麟7系列将至，在5G上的爆发力不容小觑](https://mp.weixin.qq.com/s/7YvQtOE_HEaekPaDDW7OOQ)，[华为P40系列也发布了](https://mp.weixin.qq.com/s/5N-MASmVm2d-xA_h4Dz92A)。[联发科技也发文大力**PRML**自家天玑系列5G芯片搭载的AI处理器APU3.0](https://mp.weixin.qq.com/s/toQvejvnmb0uTXJPtNAgow)，但我们却迟迟不见搭载天玑1000的手机。  


## 业界新闻

- [苹果A12Z处理器揭秘：A12X打开第八个隐藏GPU核心 | 半导体行业观察](https://mp.weixin.qq.com/s/u5_4g7V-6Rl7hKk54V_j2w)  
摘要：最近发布的iPad Pro 2020款配备了一颗特殊的A12Z处理器，这也是苹果第一次使用“Z”字母作为处理器型号的后缀，那么它和上代iPad Pro使用的A12X有什么不同呢？A12X处理器采用台积电7nm工艺制造，拥有多达100亿个晶体管，集成八核心CPU、七核心GPU、神经网络引擎，每秒运算高达五万亿次，还支持先进的机器学习。  
A12Z其实在芯片层面和A12X是一模一样的，唯一主要区别就是开启了隐藏的第八个GPU核心，也就是拥有八核心CPU、八核心GPU，图形性能因此有所提升。  
另外，A12Z还在A12X的基础上优化了性能控制器，增强了散热架构，CPU核心的频率也有可能更高。  
- [搭载5G SOC麒麟820的荣耀30S即将发布 | 电脑爱好者](https://mp.weixin.qq.com/s/S4UGWqEioWvW3b4FHsddsQ)  
摘要：一颗SoC的性能几何，在很大程度上取决于制程工艺，工艺越先进，SoC就能更长时间满血运行，发热量还低，在同级别SoC中容易占据优势。
据悉，麒麟820将基于台积电最成熟的7nm工艺制造，采用Cortex-A76架构的CPU，以及Mali-G77架构的GPU。同时，麒麟820还将集成华为自研的最新达芬奇架构NPU，ISP性能更强。  
Cortex-A76和Mali-G77的组合强吗？答案是还凑合。Cortex-A76是ARM在2018年发布的CPU核心（同期GPU为Mali-G76），而Mali-G77则是ARM在2019年发布的GPU核心（同期CPU为Cortex-A77），它们的组合属于跨代的混搭，没能用A77+G77的黄金搭档是麒麟820的最大遗憾之处。  
在中高端5G SoC中，目前只有联发科天玑1000L采用了A77+G77的组合，Exyno 980是A77+G76，联发科还未上市的天玑800则是A76+G77，后者和麒麟820的思路一样。  
- [如何看待3月25号开源的旷视深度学习框架天元MegEngine | 知乎](https://www.zhihu.com/question/377416272)  
地址：https://github.com/MegEngine/MegEngine  
摘要：高效的、灵活的 LocalConv/GroupLocalConv，任意图、全自动 sublinear memory，内存不够时进行碎片整理。然而，要想成为主流，就得解决一个tensorflow和pytorch没能解决的痛点，目前看来国内的情况都不是很乐观。  
- [如何评价清华大学发布的自研深度学习框架-计图(Jittor) | 知乎](https://www.zhihu.com/question/380993685)  
地址：https://github.com/Jittor/jittor  
摘要：实现了一个比较经典的DAG graph，以及在图上来做fusion和各种pass。从op的实现上，选择了细粒度的op，例如bcast，reduce，等等，然后通过这种方式来形成meta op，比如说convolution：https://github.com/Jittor/jittor/blob/master/notebook/meta_op.src.md  
  1. 值得关注的一点是，在XLA的早期，也有过对于op粒度的探索，目前大家的一些结论是，常见的op，比如说convolution，gemm，如果用细粒度op来实现，然后这些细粒度op是在一个op graph当中来做jit的，对性能会是一个很大的挑战（除了在代码里面embed constant value，loop reordering等等）之外，很多关于计算的细节信息都丢失了，会对后面的fusion pass有很大的挑战。  
  2. 现在一般的自动编译框架选择的方式其实是选择两层IR，一层做计算图DAG，一层做数学表达（比如说bcast，reduce，最典型的是Halide）。可能值得看一看。  
- [如何看待 2020 年 3 月 28 日华为开源的深度学习框架 MindSpore | 知乎](https://www.zhihu.com/question/383135317)  
地址：https://gitee.com/mindspore/mindspore  
摘要：引用袁老师的评价：Mindspore 个人带来了惊喜，在众所周知的难题上勇闯无人区，auto-parallel完成度很高，数据并行，模型并行和混合并行。好像我应该算在社区最多鼓吹这个概念，也比较早，但并没有发表论文，近些年Google Mesh-tensorflow, gpipe也都出了论文，之前MXNet团队的Wang Minjie也发过相关论文，斯坦福的FlexFlow等都曾讨论过相关思路，尽管有这些先行者，但完整的在框架内实现出来是非常不易的。Mindspore团队集合了大学教授，2012实验室编译、分布式系统方向造诣很深的架构师，最顶级的工程师团队，既谦逊又无畏，令人敬畏，只要是好的想法，都可以为我所用，如果一个问题很重要，还没有可模仿的先例，也一定不惜任何代价搞定，有人说Mindspore团队说“是一帮狠人”，可以说是非常高的评价了。Mindspore的代码不是那么美观，类似Google style，但细节上并没有严格执行，和Tensorflow一样不嫌麻烦的抽象，这种方式适合大规模协同研发，又一定程度上保证质量。  

## 论文

- [CVPR2020] [GAN Compression：计算量减少20倍，生成效果不变，GPU、CPU统统能加速 | 量子位](https://mp.weixin.qq.com/s/X0YLUk90jPpo1mNyAUZhVw)  
文章：https://arxiv.org/abs/2003.08936  
代码：https://github.com/mit-han-lab/gan-compression  
摘要：来自MIT、Adobe研究院和上海交通大学的团队琢
